{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f9a69d3",
   "metadata": {},
   "source": [
    "Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a62fd501",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import numpy as np\n",
    "import optuna\n",
    "import os\n",
    "from pathlib import Path\n",
    "import random\n",
    "import re\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import time \n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "random_state = 42\n",
    "torch.manual_seed(random_state)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.cuda.manual_seed_all(random_state)\n",
    "\n",
    "os.makedirs('Hyperparmeter Tuning', exist_ok=True)\n",
    "os.makedirs('Model', exist_ok=True)\n",
    "os.makedirs('Plots', exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d055bd1",
   "metadata": {},
   "source": [
    "Fake GUI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb080c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 500\n",
    "split_ratio = 0.2\n",
    "batch_size = 64\n",
    "n_optuna_trials = 600\n",
    "metric = 'Accuracy'\n",
    "\n",
    "dataset_name = 'convexity_dataset'\n",
    "dataset_name = 'firearm_dataset'\n",
    "\n",
    "model_choice = 'Transformer'\n",
    "model_choice = 'CNN'\n",
    "\n",
    "if dataset_name == 'firearm_dataset':\n",
    "    n_channels = 11\n",
    "    n_classes = 4\n",
    "    seq_len = 300\n",
    "elif dataset_name == 'convexity_dataset':\n",
    "    n_samples = 10000\n",
    "    n_channels = 2\n",
    "    n_classes = 2\n",
    "    seq_len = 100\n",
    "\n",
    "CNN_model_depth_range = [1, 3]              # step: 1\n",
    "CNN_base_n_kernels_range = [4, 16]          # step: 4\n",
    "CNN_base_kernel_size_range = [3, 9]         # step: 2\n",
    "CNN_pool_size_range = [2, 4]                # step: 1\n",
    "\n",
    "Transformer_model_depth_range = [1, 5]      # step: 1\n",
    "Transformer_embedding_dim_range = [4]    # step: 1 (d = 2^(embedding_dim))\n",
    "Transformer_n_heads_range = [2]          # step: 1 (h = 2^(n_heads))\n",
    "\n",
    "learning_rate_range = [1e-7, 1e-1]          # log\n",
    "dropout_rate_range = [0.1, 0.5]             # na\n",
    "patience_range = [5, 25]                    # step: 5\n",
    "delta_range = [1e-4, 1.0]                   # log\n",
    "\n",
    "generic_params = {\n",
    "    'n_epochs': n_epochs,\n",
    "    'n_channels': n_channels,\n",
    "    'seq_len': seq_len,\n",
    "    'n_classes': n_classes,\n",
    "    'random_state': random_state\n",
    "}\n",
    "\n",
    "match model_choice:\n",
    "    case 'CNN':\n",
    "        search_space = {\n",
    "            'CNN_model_depth_range': CNN_model_depth_range,                     \n",
    "            'CNN_base_n_kernels_range': CNN_base_n_kernels_range,               \n",
    "            'CNN_base_kernel_size_range': CNN_base_kernel_size_range,           \n",
    "            'CNN_pool_size_range': CNN_pool_size_range,                         \n",
    "        }\n",
    "    case 'Transformer':\n",
    "        search_space = {\n",
    "            'Transformer_model_depth_range': Transformer_model_depth_range,     \n",
    "            'Transformer_embedding_dim_range': Transformer_embedding_dim_range, \n",
    "            'Transformer_n_heads_range': Transformer_n_heads_range              \n",
    "        }\n",
    "    case _:\n",
    "        search_space = {}\n",
    "\n",
    "search_space['learning_rate_range'] = learning_rate_range\n",
    "search_space['dropout_rate_range'] = dropout_rate_range  \n",
    "search_space['patience_range'] = patience_range\n",
    "search_space['delta_range'] = delta_range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "767d5090",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_dict = {\n",
    "    0: 'Good Grip, Good Trigger',\n",
    "    1: 'Good Grip, Bad Trigger',\n",
    "    2: 'Bad Grip, Good Trigger',\n",
    "    3: 'Bad Grip, Bad Trigger' \n",
    "}\n",
    "\n",
    "def check_for_file_duplicates(path:str) -> str:\n",
    "\n",
    "    path = Path(path)\n",
    "\n",
    "    if not path.exists():\n",
    "        return str(path)\n",
    "\n",
    "    file_name = path.stem\n",
    "    file_extension = path.suffix\n",
    "    path_directory = path.parent\n",
    "\n",
    "    counter = 1\n",
    "    while True:\n",
    "        new_name = f\"{file_name}_{counter}{file_extension}\"\n",
    "        new_path = path_directory / new_name\n",
    "        \n",
    "        if not new_path.exists():\n",
    "            return str(new_path)\n",
    "        \n",
    "        counter += 1\n",
    "\n",
    "def convex_generator(seq_len: int, noise_level: float = 0.1) -> torch.tensor:\n",
    "    \n",
    "    x = torch.linspace(-1, 1, seq_len)\n",
    "    \n",
    "    a = abs(torch.randn(1).item())\n",
    "    b = torch.randn(1).item() * 0.5  \n",
    "    c = torch.randn(1).item() \n",
    "    \n",
    "    y = a * (x - b) ** 2 + c\n",
    "    \n",
    "    noise = torch.randn(seq_len) * noise_level\n",
    "    y = y + noise\n",
    "    \n",
    "    return y.view(1, seq_len)\n",
    "\n",
    "def concave_generator(seq_len: int, noise_level: float = 0.1) -> torch.tensor:\n",
    "    \n",
    "    x = torch.linspace(-1, 1, seq_len)\n",
    "    \n",
    "    a = abs(torch.randn(1).item())\n",
    "    b = torch.randn(1).item() * 0.5 \n",
    "    c = torch.randn(1).item()\n",
    "    \n",
    "    y = -(a * (x - b) ** 2 + c)\n",
    "    \n",
    "    noise = torch.randn(seq_len) * noise_level\n",
    "    y = y + noise\n",
    "    \n",
    "    return y.view(1, seq_len)\n",
    "\n",
    "def create_two_channel_dataset(n_samples:int, seq_len:int) -> TensorDataset:\n",
    "    \n",
    "    data = []\n",
    "    labels = []\n",
    "    generators = [(convex_generator, \"convex\"), (concave_generator, \"concave\")]\n",
    "    \n",
    "    for _ in range(n_samples):\n",
    "        gen1, type1 = random.choice(generators)\n",
    "        gen2, type2 = random.choice(generators)\n",
    "        \n",
    "        channel1 = gen1(seq_len)\n",
    "        channel2 = gen2(seq_len)\n",
    "        \n",
    "        sample = torch.cat([channel1, channel2], dim=0)\n",
    "        data.append(sample)\n",
    "        \n",
    "        label = 1 if type1 == type2 else 0\n",
    "        labels.append(label)\n",
    "\n",
    "        if _ < 5:\n",
    "            print(f'Channel 1: {type1}')\n",
    "            print(f'Channel 2: {type2}')\n",
    "            channel1 = sample[0, :].cpu().numpy()  # Channel 1 values, shape: [seq_length]\n",
    "            channel2 = sample[1, :].cpu().numpy()  # Channel 2 values, shape: [seq_length]\n",
    "            indices = list(range(1, sample.shape[1] + 1))\n",
    "            print(f'Label:     {label} (1 represents same, 0 represents different)')\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            plt.plot(indices, channel1, label='Channel 1', color='blue', linewidth=2)\n",
    "            plt.plot(indices, channel2, label='Channel 2', color='red', linewidth=2)\n",
    "            plt.xlabel('Index')\n",
    "            plt.ylabel('Value')\n",
    "            plt.title(f'Two Channel Dataset Instance {_}')\n",
    "            plt.legend()\n",
    "            plt.grid(True)\n",
    "            plt.show()\n",
    "    \n",
    "    features_tensor = torch.stack(data, dim=0)\n",
    "    labels_tensor = torch.tensor(labels, dtype=torch.long).view(-1, 1)\n",
    "\n",
    "    return (features_tensor, labels_tensor)\n",
    "\n",
    "def generate_convexity_dataset(file_name, num_samples:int, seq_len:int) -> None:\n",
    "    \n",
    "    if not os.path.exists(f'{file_name}.npz'):\n",
    "        features, labels = create_two_channel_dataset(num_samples, seq_len)\n",
    "        np.savez(f'{file_name}.npz', features=features.numpy(), labels=labels.numpy())\n",
    "        print(f\"Dataset saved to {file_name}.\")\n",
    "    else:\n",
    "        print(f\"Dataset already exists at {file_name}.\")\n",
    "\n",
    "def load_dataset(file_name, n_channels) -> TensorDataset:\n",
    "    \n",
    "    with np.load(f'Segmented Dataset/{file_name}.npz') as f:\n",
    "        features = torch.tensor(f['features'], dtype=torch.float32)\n",
    "        features = features[:, :n_channels, :]\n",
    "        print(f'Shape of features: {features.shape}')\n",
    "        labels = torch.tensor(f['labels'], dtype=torch.long)\n",
    "    dataset = TensorDataset(features, labels)\n",
    "    print(f\"Loaded dataset with {len(dataset)} samples.\")\n",
    "    return dataset\n",
    "\n",
    "def plot_confusion_matrix(predictions: np.array, labels: np.array, dataloader_name: str='given dataloader'):\n",
    "    \n",
    "    cm = confusion_matrix(predictions, labels)\n",
    "    accuracy = accuracy_score(predictions, labels)\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=[f'{class_dict[i]}' for i in range(4)], \n",
    "                yticklabels=[f'{class_dict[i]}' for i in range(4)])\n",
    "    plt.xlabel('Model Prediction', fontsize=10)\n",
    "    plt.ylabel('Ground Truth', fontsize=10)\n",
    "    plt.xticks(rotation=45, ha='center', rotation_mode='default', fontsize=7)\n",
    "    plt.yticks(rotation=45, fontsize=7)\n",
    "    colorbar = plt.gcf().axes[-1]\n",
    "    colorbar.yaxis.set_major_locator(ticker.MaxNLocator(integer=True))\n",
    "    plt.title(f'Confusion Matrix on {dataloader_name} \\n ({len(predictions)} samples, {accuracy * 100:.2f}% accuracy)', fontsize = 14)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'Plots/{model_choice}_{dataset_name.split('_')[0]}_confusion_matrix_{dataloader_name.split()[0].split('_')[0]}.png', dpi=600)\n",
    "    plt.show()\n",
    "\n",
    "def plot_train_history(train_history:list):\n",
    "\n",
    "    (train_losses, train_accuracies), (val_losses, val_accuracies) = train_history\n",
    "\n",
    "    plt.figure(figsize=(12, 5))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(range(1, len(train_losses) + 1), train_losses, label='Train Loss', marker='o')\n",
    "    plt.plot(range(1, len(train_losses) + 1), val_losses, label='Validation Loss', marker='o')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title(f'Training and Validation Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(range(1, len(train_losses) + 1), train_accuracies, label='Train Accuracy', marker='o')\n",
    "    plt.plot(range(1, len(train_losses) + 1), val_accuracies, label='Validation Accuracy', marker='o')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy (%)')\n",
    "    plt.title(f'Training and Validation Accuracy')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'Plots/{model_choice}_{dataset_name.split('_')[0]}_train_history.png', dpi=600)\n",
    "    plt.show()\n",
    "\n",
    "def scan_best_hyperparams(optuna_file_path: str):\n",
    "    with open(optuna_file_path, \"r\") as f:\n",
    "        content = f.read()\n",
    "    \n",
    "    start_index = content.index(\"Optuna Tuning:\")\n",
    "    optuna_tuning = content[start_index:]\n",
    "    trials = re.findall(r'Best parameters: \\{(.*?)\\}', optuna_tuning)\n",
    "\n",
    "    best_hyperparams = {}  \n",
    "\n",
    "    for params in trials:\n",
    "        for param in params.split(\", \"):\n",
    "            inner_key, val = param.split(\": \")\n",
    "            inner_key = inner_key.strip(\"'\")\n",
    "            if val.isdigit():\n",
    "                best_hyperparams[inner_key] = float(val)\n",
    "            else:\n",
    "                best_hyperparams[inner_key] = val.strip(\"'\")\n",
    "\n",
    "    match model_choice:\n",
    "        case 'CNN':\n",
    "            best_hyperparams['model_depth'] = int(best_hyperparams['model_depth'])\n",
    "            best_hyperparams['base_n_kernels'] = int(best_hyperparams['base_n_kernels'])\n",
    "            best_hyperparams['base_kernel_size'] = int(best_hyperparams['base_kernel_size'])\n",
    "            best_hyperparams['pool_size'] = int(best_hyperparams['pool_size'])\n",
    "            best_hyperparams['dropout_rate'] = float(best_hyperparams['dropout_rate'])\n",
    "            best_hyperparams['learning_rate'] = float(best_hyperparams['learning_rate'])\n",
    "            best_hyperparams['patience'] = int(best_hyperparams['patience'])\n",
    "            best_hyperparams['delta'] = float(best_hyperparams['delta'])\n",
    "        case 'Transformer':\n",
    "            best_hyperparams['model_depth'] = int(best_hyperparams['model_depth'])\n",
    "            best_hyperparams['embedding_dim'] = int(best_hyperparams['embedding_dim'])\n",
    "            best_hyperparams['n_heads'] = int(best_hyperparams['n_heads'])\n",
    "            best_hyperparams['learning_rate'] = float(best_hyperparams['learning_rate'])\n",
    "            best_hyperparams['patience'] = int(best_hyperparams['patience'])\n",
    "            best_hyperparams['delta'] = float(best_hyperparams['delta'])\n",
    "\n",
    "    return best_hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af67cde5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, patience=10, delta=0.001):\n",
    "        self.patience = patience\n",
    "        self.delta = delta\n",
    "        self.best_loss = None \n",
    "        self.counter = 0\n",
    "        self.early_stop = False\n",
    "        self.optimal_val_accuracy = 0.0 \n",
    "        self.optimal_epoch = 0\n",
    "        self.best_state_dict = None\n",
    "\n",
    "    def __call__(self, val_loss, val_accuracy, epoch, model):\n",
    "        if self.best_loss is None:\n",
    "            self.best_loss = val_loss\n",
    "            self.optimal_val_accuracy = val_accuracy\n",
    "            self.optimal_epoch = epoch\n",
    "            self.best_state_dict = copy.deepcopy(model.state_dict())\n",
    "        elif val_loss > self.best_loss - self.delta:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_loss = val_loss\n",
    "            self.optimal_val_accuracy = val_accuracy\n",
    "            self.optimal_epoch = epoch\n",
    "            self.best_state_dict = copy.deepcopy(model.state_dict())\n",
    "            self.counter = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e9805a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN1D(nn.Module):\n",
    "\n",
    "    def __init__(self, \n",
    "                n_epochs: int,\n",
    "                input_channels: int, \n",
    "                seq_length:int, \n",
    "                num_classes: int, \n",
    "                model_depth: int, \n",
    "                base_n_kernels: int, \n",
    "                base_kernel_size: int, \n",
    "                pool_size: int, \n",
    "                dropout_rate: float, \n",
    "                random_state:int = 42\n",
    "        ):\n",
    "\n",
    "        random.seed(random_state)\n",
    "        np.random.seed(random_state)\n",
    "        torch.manual_seed(random_state)\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.manual_seed_all(random_state)\n",
    "            torch.backends.cudnn.deterministic = True\n",
    "            torch.backends.cudnn.benchmark = False\n",
    "        \n",
    "        super(CNN1D, self).__init__()\n",
    "        \n",
    "        self.model_depth = model_depth\n",
    "        self.n_epochs = n_epochs\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.conv_layers = nn.ModuleList()\n",
    "        self.relu_layers = nn.ModuleList()\n",
    "        self.pool_layers = nn.ModuleList()\n",
    "\n",
    "        for layer_idx in range(model_depth):\n",
    "            self.conv_layers.append(nn.Conv1d(in_channels=input_channels if layer_idx==0 else base_n_kernels*(layer_idx), out_channels=base_n_kernels*(layer_idx+1), kernel_size=base_kernel_size, padding=1))\n",
    "            self.relu_layers.append(nn.ReLU())\n",
    "            self.pool_layers.append(nn.MaxPool1d(kernel_size=pool_size))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        \n",
    "        def calc_output_size(input_size, kernel_size, stride, padding):\n",
    "            return (input_size + 2 * padding - kernel_size) // stride + 1\n",
    "        \n",
    "        tensor_size = seq_length\n",
    "        for layer_idx in range(model_depth):\n",
    "            tensor_size = calc_output_size(tensor_size, kernel_size=base_kernel_size, stride=1, padding=1)  \n",
    "            tensor_size = calc_output_size(tensor_size, kernel_size=pool_size, stride=pool_size, padding=0)  \n",
    "        self.fc = nn.Linear(base_n_kernels * model_depth * tensor_size, num_classes)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def set_Adam_learning_rate(self, lr: float):\n",
    "\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=lr)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        for layer_idx in range(self.model_depth):\n",
    "            x = self.conv_layers[layer_idx](x)\n",
    "            x = self.relu_layers[layer_idx](x)\n",
    "            x = self.pool_layers[layer_idx](x)\n",
    "        x = self.dropout(x)\n",
    "        x = x.view(x.size(0), -1)  \n",
    "        x = self.fc(x)  \n",
    "\n",
    "        return x\n",
    "    \n",
    "    def train_model(self, train_loader: DataLoader, val_loader: DataLoader, patience: int, delta: float, verbose: bool=False, save: bool=False):\n",
    "\n",
    "        train_losses = []\n",
    "        train_accuracies = []\n",
    "        val_losses = []\n",
    "        val_accuracies = []\n",
    "\n",
    "        early_stopping = EarlyStopping(patience=patience, delta=delta)\n",
    "\n",
    "        for epoch in range(1, self.n_epochs + 1):\n",
    "            \n",
    "            self.train()\n",
    "            train_loss = 0.0\n",
    "            train_correct = 0\n",
    "            train_total = 0\n",
    "\n",
    "            for features, labels in train_loader:\n",
    "                features, labels = features.to(device), labels.to(device)\n",
    "\n",
    "                labels = labels.squeeze(-1)\n",
    "\n",
    "                self.optimizer.zero_grad()\n",
    "                outputs = self(features)\n",
    "                loss = self.criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                \n",
    "                train_loss += loss.item()\n",
    "                _, predictions = torch.max(outputs.data, 1)\n",
    "                train_total += labels.size(0)\n",
    "                train_correct += (predictions == labels).sum().item()\n",
    "            \n",
    "            train_loss = train_loss / len(train_loader)\n",
    "            train_accuracy = 100 * train_correct / train_total\n",
    "            train_losses.append(train_loss)\n",
    "            train_accuracies.append(train_accuracy)\n",
    "            \n",
    "            self.eval()\n",
    "            \n",
    "            val_loss = 0.0\n",
    "            val_correct = 0\n",
    "            val_total = 0\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for features, labels in val_loader:\n",
    "                    features, labels = features.to(device), labels.to(device)\n",
    "                    outputs = self(features)\n",
    "                    labels = labels.squeeze(-1)\n",
    "\n",
    "                    loss = self.criterion(outputs, labels)\n",
    "                    \n",
    "                    val_loss += loss.item()\n",
    "                    _, predictions = torch.max(outputs.data, dim=1)\n",
    "\n",
    "                    val_total += labels.size(0)\n",
    "                    val_correct += (predictions == labels).sum().item()\n",
    "\n",
    "            val_loss = val_loss / len(val_loader)\n",
    "            val_accuracy = 100 * val_correct / val_total\n",
    "            val_losses.append(val_loss)\n",
    "            val_accuracies.append(val_accuracy)\n",
    "            \n",
    "            print(f'Epoch [{epoch}/{self.n_epochs}], '\n",
    "                f'Train Loss: {train_loss:.4f}, Train Acc: {train_accuracy:.2f}%, '\n",
    "                f'Val Loss: {val_loss:.4f}, Val Acc: {val_accuracy:.2f}%') if verbose else None\n",
    "\n",
    "            early_stopping(val_loss=val_loss, val_accuracy=val_accuracy, epoch=epoch, model=self)\n",
    "            if early_stopping.early_stop:\n",
    "                break\n",
    "\n",
    "        if verbose == True:\n",
    "            print(f'EarlyStopping optimal epoch: {early_stopping.optimal_epoch}')\n",
    "            train_losses = train_losses[:early_stopping.optimal_epoch]\n",
    "            train_accuracies = train_accuracies[:early_stopping.optimal_epoch]\n",
    "            val_losses = val_losses[:early_stopping.optimal_epoch]\n",
    "            val_accuracies = val_accuracies[:early_stopping.optimal_epoch]\n",
    "            train_history = (train_losses, train_accuracies), (val_losses, val_accuracies)\n",
    "            plot_train_history(train_history=train_history)\n",
    "        \n",
    "        self.load_state_dict(early_stopping.best_state_dict)\n",
    "        \n",
    "        if save == True:\n",
    "            torch.save(self.state_dict(), f'Model/{model_choice}_{dataset_name.split('_')[0]}.pt')\n",
    "\n",
    "        return early_stopping.optimal_val_accuracy\n",
    "\n",
    "    def perform_inference(self, any_loader: DataLoader, dataloader_name: str='given loader', output:str = None):\n",
    "        \n",
    "        self.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        all_predictions = []\n",
    "        all_labels = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for features, labels in any_loader:\n",
    "                features, labels = features.to(device), labels.to(device)\n",
    "                outputs = self(features)\n",
    "                labels = labels.squeeze(-1)\n",
    "\n",
    "                _, predictions = torch.max(outputs.data, dim=1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predictions == labels).sum().item()\n",
    "\n",
    "            all_predictions.append(predictions)\n",
    "            all_predictions_tensor = torch.cat(all_predictions, dim=0)\n",
    "            all_labels.append(labels)\n",
    "            all_labels_tensor = torch.cat(all_labels, dim=0)\n",
    "\n",
    "            print(f'Inference accuracy on {dataloader_name}: {100 * correct / total:.2f}%')\n",
    "            \n",
    "            if output in ['numpy', 'np', 'np array', 'nparray']:\n",
    "                return all_predictions_tensor.cpu().numpy(), all_labels_tensor.cpu().numpy()\n",
    "\n",
    "        return (all_predictions_tensor, all_labels_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecd8f28d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, embedding_dim: int, seq_len: int, dropout_rate: float):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout_rate)\n",
    "        pe = torch.zeros(seq_len, embedding_dim)\n",
    "        position = torch.arange(0, seq_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, embedding_dim, 2).float() * (-math.log(10000.0) / embedding_dim))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = x + self.pe[:, :x.size(1), :].to(x.device)\n",
    "        return self.dropout(x)\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "        \n",
    "    def __init__(self, \n",
    "                n_epochs: int, \n",
    "                n_channels: int, \n",
    "                n_classes: int, \n",
    "                seq_len: int, \n",
    "                model_depth: int, \n",
    "                embedding_dim: int, \n",
    "                n_heads: int, \n",
    "                dropout_rate: int,\n",
    "                random_state: int=42):\n",
    "        \n",
    "        random.seed(random_state)\n",
    "        np.random.seed(random_state)\n",
    "        torch.manual_seed(random_state)\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.manual_seed_all(random_state)\n",
    "            torch.backends.cudnn.deterministic = True\n",
    "            torch.backends.cudnn.benchmark = False\n",
    "        \n",
    "        super().__init__()\n",
    "\n",
    "        self.n_epochs = n_epochs\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        \n",
    "        self.embedding = nn.Linear(n_channels, embedding_dim)\n",
    "        self.pos_encoder = PositionalEncoding(embedding_dim, seq_len, dropout_rate)\n",
    "        self.transformer = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(embedding_dim, n_heads), model_depth\n",
    "        )\n",
    "        self.fc = nn.Linear(embedding_dim, n_classes)\n",
    "\n",
    "    def set_Adam_learning_rate(self, lr: float):\n",
    "\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=lr)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.transpose(1, 2)\n",
    "        x = self.embedding(x)  \n",
    "        x = self.pos_encoder(x)  \n",
    "        x = self.transformer(x)  \n",
    "        x = x[:, -1, :] \n",
    "        x = self.fc(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def train_model(self, train_loader: DataLoader, val_loader: DataLoader, patience: int, delta: float, verbose: bool=False, save: bool=False):\n",
    "\n",
    "        train_losses = []\n",
    "        train_accuracies = []\n",
    "        val_losses = []\n",
    "        val_accuracies = []\n",
    "\n",
    "        early_stopping = EarlyStopping(patience=patience, delta=delta)\n",
    "\n",
    "        for epoch in range(1, self.n_epochs + 1):\n",
    "            \n",
    "            self.train()\n",
    "            train_loss = 0.0\n",
    "            train_correct = 0\n",
    "            train_total = 0\n",
    "\n",
    "            for features, labels in train_loader:\n",
    "                features, labels = features.to(device), labels.to(device)\n",
    "\n",
    "                labels = labels.squeeze(-1)\n",
    "\n",
    "                self.optimizer.zero_grad()\n",
    "                outputs = self(features)\n",
    "                loss = self.criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                \n",
    "                train_loss += loss.item()\n",
    "                _, predictions = torch.max(outputs.data, 1)\n",
    "\n",
    "                # print('')\n",
    "                # print(f'train labels:      {labels}')\n",
    "                # print(f'train predictions: {predictions}')\n",
    "\n",
    "                train_total += labels.size(0)\n",
    "                train_correct += (predictions == labels).sum().item()\n",
    "            \n",
    "            train_loss = train_loss / len(train_loader)\n",
    "            train_accuracy = 100 * train_correct / train_total\n",
    "            train_losses.append(train_loss)\n",
    "            train_accuracies.append(train_accuracy)\n",
    "            \n",
    "            self.eval()\n",
    "            \n",
    "            val_loss = 0.0\n",
    "            val_correct = 0\n",
    "            val_total = 0\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for features, labels in val_loader:\n",
    "                    features, labels = features.to(device), labels.to(device)\n",
    "                    outputs = self(features)\n",
    "                    labels = labels.squeeze(-1)\n",
    "\n",
    "                    loss = self.criterion(outputs, labels)\n",
    "                    \n",
    "                    val_loss += loss.item()\n",
    "                    _, predictions = torch.max(outputs.data, dim=1)\n",
    "\n",
    "                    # print('')\n",
    "                    # print(f'val labels:      {labels}')\n",
    "                    # print(f'val predictions: {predictions}')\n",
    "\n",
    "                    val_total += labels.size(0)\n",
    "                    val_correct += (predictions == labels).sum().item()\n",
    "\n",
    "            val_loss = val_loss / len(val_loader)\n",
    "            val_accuracy = 100 * val_correct / val_total\n",
    "            val_losses.append(val_loss)\n",
    "            val_accuracies.append(val_accuracy)\n",
    "            \n",
    "            print(f'Epoch [{epoch}/{self.n_epochs}], '\n",
    "                f'Train Loss: {train_loss:.4f}, Train Acc: {train_accuracy:.2f}%, '\n",
    "                f'Val Loss: {val_loss:.4f}, Val Acc: {val_accuracy:.2f}%') if verbose else None\n",
    "\n",
    "            early_stopping(val_loss=val_loss, val_accuracy=val_accuracy, epoch=epoch, model=self)\n",
    "            if early_stopping.early_stop:\n",
    "                break\n",
    "\n",
    "        if verbose == True:\n",
    "            print(f'EarlyStopping optimal epoch: {early_stopping.optimal_epoch}')\n",
    "            train_losses = train_losses[:early_stopping.optimal_epoch]\n",
    "            train_accuracies = train_accuracies[:early_stopping.optimal_epoch]\n",
    "            val_losses = val_losses[:early_stopping.optimal_epoch]\n",
    "            val_accuracies = val_accuracies[:early_stopping.optimal_epoch]\n",
    "            train_history = (train_losses, train_accuracies), (val_losses, val_accuracies)\n",
    "            plot_train_history(train_history=train_history)\n",
    "        \n",
    "        self.load_state_dict(early_stopping.best_state_dict)\n",
    "        \n",
    "        if save == True:\n",
    "            torch.save(self.state_dict(), f'Model/{model_choice}_{dataset_name.split('_')[0]}.pt')\n",
    "\n",
    "        return early_stopping.optimal_val_accuracy\n",
    "\n",
    "    def perform_inference(self, any_loader: DataLoader, dataloader_name: str='given loader', output:str = None):\n",
    "        \n",
    "        self.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        all_predictions = []\n",
    "        all_labels = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for features, labels in any_loader:\n",
    "                features, labels = features.to(device), labels.to(device)\n",
    "                outputs = self(features)\n",
    "                labels = labels.squeeze(-1)\n",
    "\n",
    "                _, predictions = torch.max(outputs.data, dim=1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predictions == labels).sum().item()\n",
    "\n",
    "            all_predictions.append(predictions)\n",
    "            all_predictions_tensor = torch.cat(all_predictions, dim=0)\n",
    "            all_labels.append(labels)\n",
    "            all_labels_tensor = torch.cat(all_labels, dim=0)\n",
    "\n",
    "            print(f'Inference accuracy on {dataloader_name}: {100 * correct / total:.2f}%')\n",
    "            \n",
    "            if output in ['numpy', 'np', 'np array', 'nparray']:\n",
    "                return all_predictions_tensor.cpu().numpy(), all_labels_tensor.cpu().numpy()\n",
    "\n",
    "        return (all_predictions_tensor, all_labels_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d11e9ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial, search_space, generic_params, train_loader, val_loader):\n",
    "    match model_choice:\n",
    "        case 'CNN':\n",
    "            param_space = {\n",
    "                'model_depth': {'type': 'int', 'range': search_space['CNN_model_depth_range'], 'step': 1},\n",
    "                'base_n_kernels': {'type': 'int', 'range': search_space['CNN_base_n_kernels_range'], 'step': 4},\n",
    "                'base_kernel_size': {'type': 'int', 'range': search_space['CNN_base_kernel_size_range'], 'step': 2},\n",
    "                'pool_size': {'type': 'int', 'range': search_space['CNN_pool_size_range'], 'step': 1},\n",
    "                'dropout_rate': {'type': 'float', 'range': search_space['dropout_rate_range']},\n",
    "                'learning_rate': {'type': 'float', 'range': search_space['learning_rate_range'], 'log': True},\n",
    "                'patience': {'type': 'int', 'range': search_space['patience_range'], 'step': 5},\n",
    "                'delta': {'type': 'float', 'range': search_space['delta_range'], 'log': True},\n",
    "            }\n",
    "        case 'Transformer':\n",
    "            param_space = {\n",
    "                'model_depth': {'type': 'int', 'range': search_space['Transformer_model_depth_range'], 'step': 1},\n",
    "                'embedding_dim': {'type': 'int', 'range': search_space['Transformer_embedding_dim_range'], 'step': 1},\n",
    "                'n_heads': {'type': 'int', 'range': search_space['Transformer_n_heads_range'], 'step': 1},\n",
    "                'dropout_rate': {'type': 'float', 'range': search_space['dropout_rate_range']},\n",
    "                'learning_rate': {'type': 'float', 'range': search_space['learning_rate_range'], 'log': True},\n",
    "                'patience': {'type': 'int', 'range': search_space['patience_range'], 'step': 5},\n",
    "                'delta': {'type': 'float', 'range': search_space['delta_range'], 'log': True},\n",
    "            }\n",
    "    params = {}\n",
    "    for name, config in param_space.items():\n",
    "        params[name] = (\n",
    "            trial.suggest_int(name, config['range'][0], config['range'][-1], step=config.get('step', 1), log=config.get('log', False)) if config['type'] == 'int' else\n",
    "            trial.suggest_float(name, config['range'][0], config['range'][-1], log=config.get('log', False)) if config['type'] == 'float' else\n",
    "            trial.suggest_categorical(name, config['range'])\n",
    "        )\n",
    "        \n",
    "    match model_choice:\n",
    "        case 'CNN':\n",
    "            model = CNN1D(n_epochs = generic_params['n_epochs'],\n",
    "                        input_channels=generic_params['n_channels'], \n",
    "                        seq_length=generic_params['seq_len'], \n",
    "                        num_classes=generic_params['n_classes'], \n",
    "                        model_depth=params['model_depth'],\n",
    "                        base_n_kernels=params['base_n_kernels'], \n",
    "                        base_kernel_size=params['base_kernel_size'], \n",
    "                        pool_size=params['pool_size'], \n",
    "                        dropout_rate=params['dropout_rate'],\n",
    "                        random_state=generic_params['random_state'])\n",
    "        case 'Transformer':\n",
    "            if params['embedding_dim'] < params['n_heads']:\n",
    "                print(f'prior to adjustment {params['embedding_dim']}, {params['n_heads']}')\n",
    "                print(params)\n",
    "                print(trial.suggest_int('n_heads', 1, 1))\n",
    "                # params['n_heads'] = trial.suggest_int('n_heads', 1, params['embedding_dim'])\n",
    "                params['n_heads'] = 1\n",
    "                print(params)\n",
    "                print(f'after adjustment {params['embedding_dim']}, {params['n_heads']}')\n",
    "            model = Transformer(n_epochs = generic_params['n_epochs'],\n",
    "                        n_channels=generic_params['n_channels'], \n",
    "                        seq_len=generic_params['seq_len'], \n",
    "                        n_classes=generic_params['n_classes'], \n",
    "                        model_depth=params['model_depth'],\n",
    "                        embedding_dim=2**params['embedding_dim'],\n",
    "                        n_heads=2**params['n_heads'],\n",
    "                        dropout_rate=params['dropout_rate'],\n",
    "                        random_state=generic_params['random_state'])\n",
    "    model.to(device)\n",
    "\n",
    "    model.set_Adam_learning_rate(lr=params['learning_rate'])\n",
    "\n",
    "    best_accuracy = model.train_model(train_loader=train_loader, val_loader=val_loader, patience=params['patience'], delta=params['delta'], verbose=0)\n",
    "\n",
    "    return best_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c84ca41",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    print(f'model_choice: {model_choice}')\n",
    "\n",
    "    generate_convexity_dataset(file_name=dataset_name, num_samples=n_samples, seq_len=seq_len) if dataset_name == 'convexity_dataset' else None\n",
    "        # print(f'Shape of features: {f['features'].shape}')\n",
    "        \n",
    "    dataset = load_dataset(file_name=dataset_name, n_channels=n_channels)\n",
    "    features, labels = dataset.tensors\n",
    "\n",
    "    train_val_indices, test_indices = train_test_split(\n",
    "        range(len(dataset)),\n",
    "        test_size=split_ratio,\n",
    "        stratify=labels,\n",
    "        random_state=random_state\n",
    "    )\n",
    "\n",
    "    train_indices, val_indices = train_test_split(\n",
    "        train_val_indices,\n",
    "        test_size=split_ratio / (1 - split_ratio),\n",
    "        stratify=[labels[i] for i in train_val_indices],\n",
    "        random_state=random_state\n",
    "    )\n",
    "\n",
    "    train_dataset = TensorDataset(features[train_indices], labels[train_indices])\n",
    "    val_dataset = TensorDataset(features[val_indices], labels[val_indices])\n",
    "    test_dataset = TensorDataset(features[test_indices], labels[test_indices])\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    start_time = time.perf_counter()\n",
    "\n",
    "    if not os.path.exists(f'Hyperparmeter Tuning/{model_choice}_{dataset_name.split('_')[0]}_optuna.txt'):\n",
    "        study = optuna.create_study(direction='maximize', sampler=optuna.samplers.TPESampler(seed=random_state))\n",
    "        study.optimize(lambda trial: objective(trial, search_space=search_space, generic_params=generic_params, train_loader=train_loader, val_loader=val_loader), n_optuna_trials)\n",
    "        with open(check_for_file_duplicates(f'Hyperparmeter Tuning/{model_choice}_{dataset_name.split('_')[0]}_optuna.txt'), 'w') as file:\n",
    "\n",
    "            file.write(\"Optuna Tuning:\")\n",
    "            file.write(\"\\n\\n\")\n",
    "            file.write(f\"Best value ({metric}): {study.best_value}\")\n",
    "            file.write(\"\\n\")\n",
    "            file.write(f\"Best parameters: {study.best_params}\")\n",
    "\n",
    "            if study.best_trial.user_attrs:\n",
    "                file.write(\"\\n\\nAdditional Trial Attributes:\")\n",
    "            for key, value in study.best_trial.user_attrs.items():\n",
    "                file.write(f\"\\n{key}: {value}\")\n",
    "            \n",
    "        print(f'Optuna files for model {model_choice} saved successfuly.')\n",
    "\n",
    "    best_hyperparams = scan_best_hyperparams(f'Hyperparmeter Tuning/{model_choice}_{dataset_name.split('_')[0]}_optuna.txt')\n",
    "    \n",
    "    match model_choice:\n",
    "        case 'CNN':\n",
    "            model = CNN1D(n_epochs=n_epochs,\n",
    "                        input_channels=n_channels, \n",
    "                        seq_length=seq_len, \n",
    "                        num_classes=n_classes, \n",
    "                        model_depth=int(best_hyperparams['model_depth']),\n",
    "                        base_n_kernels=best_hyperparams['base_n_kernels'], \n",
    "                        base_kernel_size=best_hyperparams['base_kernel_size'], \n",
    "                        pool_size=best_hyperparams['pool_size'], \n",
    "                        dropout_rate=best_hyperparams['dropout_rate'],\n",
    "                        random_state=random_state)\n",
    "        case 'Transformer':\n",
    "            model = Transformer(n_epochs=n_epochs, \n",
    "                                n_channels=n_channels, \n",
    "                                seq_len=seq_len, \n",
    "                                n_classes=n_classes,\n",
    "                                model_depth=int(best_hyperparams['model_depth']), \n",
    "                                embedding_dim=2**int(best_hyperparams['embedding_dim']), \n",
    "                                n_heads=2**int(best_hyperparams['n_heads']), \n",
    "                                dropout_rate=float(best_hyperparams['dropout_rate']),\n",
    "                                random_state=random_state)\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    if not os.path.exists(f'Model/{model_choice}_{dataset_name.split('_')[0]}.pt'):\n",
    "        model.set_Adam_learning_rate(lr=float(best_hyperparams['learning_rate']))\n",
    "        best_accuracy = model.train_model(train_loader=train_loader, val_loader=val_loader, patience=int(best_hyperparams['patience']), delta=float(best_hyperparams['delta']), verbose=1, save=True)\n",
    "        train_predictions, train_labels = model.perform_inference(any_loader=train_loader, dataloader_name='Train Loader', output='np')\n",
    "        val_predictions, val_labels = model.perform_inference(any_loader=val_loader, dataloader_name='Validation Loader', output='np')\n",
    "        test_predictions, test_labels = model.perform_inference(any_loader=test_loader, dataloader_name='Test Loader', output='np')\n",
    "    else:\n",
    "        model.load_state_dict(torch.load(f'Model/{model_choice}_{dataset_name.split('_')[0]}.pt', map_location=device))\n",
    "        train_predictions, train_labels = model.perform_inference(any_loader=train_loader, dataloader_name='Train Loader', output='np')\n",
    "        val_predictions, val_labels = model.perform_inference(any_loader=val_loader, dataloader_name='Validation Loader', output='np')\n",
    "        test_predictions, test_labels = model.perform_inference(any_loader=test_loader, dataloader_name='Test Loader', output='np')\n",
    "\n",
    "    plot_confusion_matrix(predictions=train_predictions, labels=train_labels, dataloader_name='Train Loader')\n",
    "    plot_confusion_matrix(predictions=val_predictions, labels=val_labels, dataloader_name='Validation Loader')\n",
    "    plot_confusion_matrix(predictions=test_predictions, labels=test_labels, dataloader_name='Test Loader')\n",
    "\n",
    "    end_time = time.perf_counter()\n",
    "    elapsed_time = end_time - start_time\n",
    "    print(f\"Elapsed time for ML pipeline: {elapsed_time:.6f} seconds\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
